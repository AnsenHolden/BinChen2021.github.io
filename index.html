<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Bin Chen</title>
  
  <meta name="author" content="Chen bin">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:820px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Bin Chen</name>
              </p>
              <p>I am currently an Assistant Professor with the Department of Computer Science and Technology<a href="http://cs.hitsz.edu.cn">Harbin Institute of Technology</a>, <a href="https://www.hitsz.edu.cn/index.html">Shenzhen (HITSZ).</a> I got my Ph.D. from the Department of Computer Science and Technology, <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a> under the supervision of  <a href="https://scholar.google.com/citations?user=koAXTXgAAAAJ">Prof. Shu-Tao Xia </a>. I have also been fortunate to visit <a href="https://uwaterloo.ca/electrical-computer-engineering/profile/ehyang">Prof. En-Hui Yang </a> at the University of Waterloo (UW) from Dec 2019 to May 2020.
                My research interests include Coding theory and Information Theory, Machine Learning and Deep Learning.</p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
	      <img style="width:80%;max-width:80%" alt="profile photo" src="images/binchen.jpg">
            </td>
          </tr>
        </tbody></table>
		
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
	      <li style="margin: 5px;" >
                <b>2023-02:</b> Two papers on human activity understanding got accepted by CVPR 2023.
              </li>
	      <li style="margin: 5px;" >
                <b>2023-01:</b> One paper on instructional video analysis got accepted by ICLR 2023.
              </li>
	      <li style="margin: 5px;" >
                <b>2023-01:</b> One paper on referring segmentation was selected as ORAL presentation by AAAI 2023.
              </li>     
	      <li style="margin: 5px;" >
                <b>2022-10:</b> Our team led by <a href="https://workforai.github.io">Yong Liu</a> (incoming Ph.D. student of IVG@SZ) won the 1st place in the <a href="https://motcomplex.github.io">Long Video Instance Segmentation Challenge</a> (ECCV 2022 Workshop).
              </li>
	      <li style="margin: 5px;" >
                <b>2022-09:</b> Two papers got accepted by NeurIPS 2022.
              </li>
	      <li style="margin: 5px;" >
                <b>2022-07:</b> <a href="https://arxiv.org/pdf/2203.10790.pdf">ScalableViT</a> and <a href="https://github.com/workforai/GSFM">GSFM</a> got accepted by ECCV 2022.
              </li>
              <li style="margin: 5px;" >
                <b>2022-04:</b> A talk at MSRA about <a href="data/LAVT-Yansong.pdf">LAVT</a>.
	      </li>
            </p>
          </td>
        </tr>
      </tbody></table>
	  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Recent Selected Publications  [<a style="font-size:20px;" href="https://scholar.google.com/citations?user=TIbistUAAAAJ&hl=zh-CN"> Full List </a>]</heading>
			  <p>(*Equal Contribution, #Corresponding Author)</p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>	

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/FLAG3D.PNG" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>FLAG3D: A 3D Fitness Activity Dataset with Language Instruction</papertitle>
              <br>
              <strong>Yansong Tang*</strong>, Jinpeng Liu*, Aoyang Liu*, Bin Yang, Wenxun Dai, Yongming Rao, Jiwen Lu, Jie Zhou, Xiu Li
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2212.04638.pdf">[arXiv (initial version)]</a>
              <a href="https://andytang15.github.io/FLAG3D/">[Project Page]</a> 
              <br>
              <p> We present FLAG3D, a large-scale 3D fitness activity dataset with language instruction.</p>
            </td>
          </tr>			

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/LOGO.PNG" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>LOGO: A Long-Form Video Dataset for Group Action Quality Assessment</papertitle>
              <br>
              Shiyi Zhang, Wenxun Dai, Sujia Wang, Xiangwei Shen, Jiwen Lu, Jie Zhou, <strong>Yansong Tang#</strong>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
              <br>
              [arXiv][Project Page](Coming soon)
              <br>
              <p> LOGO is a new multi-person long-form video dataset for action quality assessment.</p>
            </td>
          </tr>			
		
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/hornet.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions</papertitle>
              <br>
              Yongming Rao*, Wenliang Zhao*, <strong>Yansong Tang</strong>, Jie Zhou, Ser-Nam Lim, Jiwen Lu
              <br>
              <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2207.14284">[arXiv]</a>
              <a href="https://github.com/raoyongming/HorNet">[Code]</a>
              <a href="https://hornet.ivg-research.xyz/">[Project Page]</a> 
              <a href="https://mp.weixin.qq.com/s/MyMIPv-bn9wVMLABurjOUA">[中文解读]</a> 
              <br>
              <p> HorNet is a family of generic vision backbones that perform explicit high-order spatial interactions based on Recursive Gated Convolution.</p>
            </td>
          </tr>		
		
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/OrdinalCLIP.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>OrdinalCLIP: Learning Probabilistic Ordinal Embeddings for Uncertainty-Aware Regression</papertitle>
              <br>
	      Wanhua Li*, Xiaoke Huang*, Zheng Zhu, <strong>Yansong Tang</strong>, Xiu Li, Jiwen Lu, Jie Zhou
              <br>
              <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2206.02338">[arXiv]</a>
              <a href="https://github.com/xk-huang/OrdinalCLIP">[Code]</a>
              <a href="https://xk-huang.github.io/OrdinalCLIP">[Project Page]</a> 
              <a href="https://zhuanlan.zhihu.com/p/565034693">[中文解读]</a> 
              <br>
              <p> We present a language-powered paradigm for ordinal regression.</p>
            </td>
          </tr>	
		
		
		
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/lavt.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>LAVT: Language-Aware Vision Transformer for Referring Image Segmentation</papertitle>
              <br>
		    Zhao Yang*, Jiaqi Wang*, <strong>Yansong Tang#</strong>, Kai Chen, Hengshuang Zhao, Philip H.S. Torr
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2112.02244v2.pdf">[arxiv]</a>
	      <a href="https://github.com/yz93/LAVT-RIS">[code]</a>
	      <a href="https://mp.weixin.qq.com/s/k8IgExjxybRSscoRy-jHSg">[中文解读]</a> 	    
              <br>
              <p> We present an end-to-end hierarchical Transformer-based network for referring segmentation.</p>
            </td>
          </tr>		

	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/BNV-Fusion.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>BNV-Fusion: Dense 3D Reconstruction using Bi-level Neural Volume Fusion</papertitle>
              <br>
		    Kejie Li, <strong>Yansong Tang</strong>, Victor Adrian Prisacariu, Philip H.S. Torr
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2204.01139">[arxiv]</a>   
    	      <a href="https://github.com/likojack/bnv_fusion">[code]</a>
	      <a href="https://mp.weixin.qq.com/s/h-8O1kFxX_IVzFB6bCNrvg">[中文解读]</a> 	    
              <br>
              <p> We present Bi-level Neural Volume Fusion, which leverages recent advances in neural implicit representations and neural rendering for dense 3D reconstruction. In order to incrementally integrate new depth maps into a global neural implicit representation, we propose a novel bi-level fusion strategy that considers both efficiency and reconstruction quality by design.</p>
            </td>
          </tr>			
		
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/denseclip.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting</papertitle>
              <br>
		    Yongming Rao*, Wenliang Zhao*, Guangyi Chen, <strong>Yansong Tang</strong>, Zheng Zhu, Guan Huang, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
	      <a href="https://arxiv.org/abs/2112.01518">[arXiv]</a>
              <a href="https://github.com/raoyongming/DenseCLIP">[Code]</a>
              <a href="https://denseclip.ivg-research.xyz">[Project Page]</a> 
              <a href="https://mp.weixin.qq.com/s/fERXjGBVXzo6TaYiV2Z9ZQ">[中文解读]</a> 	    
              <br>
              <p> DenseCLIP is a new framework for dense prediction by implicitly and explicitly leveraging the pre-trained knowledge from CLIP.</p>
            </td>
          </tr>				


	
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/tpami.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Comprehensive Instructional Video Analysis: The COIN Dataset and Performance Evaluation</papertitle>
              <br>
			  <strong>Yansong Tang</strong>, Jiwen Lu, and Jie Zhou
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>, 2021
	      <br>
              <a href="https://arxiv.org/abs/2003.09392">[arXiv]</a>  
	      <a href="https://coin-dataset.github.io/">[Project Page]</a> 
              <a href="https://mp.weixin.qq.com/s/bYaAYkTj_0OxpDcWsToBWw">[中文解读]</a> 		    
              <br>
              <p> COIN is currently the largest and most comprehensive instructional video analysis datasets with rich annotations.</p>
            </td>
          </tr>			

	<tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/musdl.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Uncertainty-aware Score Distribution Learning for Action Quality Assessment</papertitle>
              <br>
			  <strong>Yansong Tang*</strong>, Zanlin Ni*, Jiahuan Zhou, Danyang Zhang, Jiwen Lu, Ying Wu, and Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2020
	      <br>
	      <font color="red"><strong>Oral Presentation</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2006.07665">[arxiv]</a> 
              <a href="https://github.com/nzl-thu/MUSDL">[Code]</a>
              <br>
              <p> We propose an uncertainty-aware score distribution learning method and extend it to a multi-path model for action quality assessment.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Teaching</heading>
            <p>
              <li style="margin: 5px;"> 
                  Data Mining: Theory and Algorithms, Fall 2022 (with <a href="https://scholar.google.com/citations?user=Ha8rlUgAAAAJ&hl=en"> Prof. Xinlei Chen</a>)
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>	      
	      
	      
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Selected Honors and Awards</heading>
            <p>
			  <li style="margin: 5px;" >		    
		    Young Elite Scientist Sponsorship Program by CAST, 2023.
              </li>		    		    
			  <li style="margin: 5px;" >		    
		    Excellent Doctoral Dissertation Award of CAAI, 2021.
              </li>		    
			  <li style="margin: 5px;" >
                Excellent PhD Graduate of Beijing, 2020.
              </li>
			  <li style="margin: 5px;" >
                Excellent Doctoral Dissertation of Tsinghua University, 2020.
              </li>
			  <li style="margin: 5px;" >
                Zijing Scholar Fellowship for Prospective Researcher, Tsinghua University, 2020.
              </li>  
            </p>
          </td>
        </tr>
      </tbody></table>

	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Group</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>PhD Students:</b>
	              <table  border=2 width = 800  bordercolor="white" align="left">
                  <tr>
                  <td width= "400">Zhiheng Li (2021-; with <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en">Prof. Jie Zhou</a>)</td>
                  <td width= "400">Yixuan Zhu (2022-; with <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en">Prof. Jie Zhou</a>)</td>
                  </tr>
                </table> 
              </li>
              <li style="margin: 5px;"> 
                <b>Master Students:</b>
                <table  border=2   width = 800 bordercolor="white" align="left">
                  <tr>
                  <td width= "400"><a href="https://xk-huang.github.io">Xiaoke Huang</a> (2021-; with <a href="https://scholar.google.com/citations?user=TN8uDQoAAAAJ&hl=en">Prof. Jiwen Lu</a>)</td>
                  <td width= "400">Rong He (2021-; with <a href="https://scholar.google.com/citations?user=TN8uDQoAAAAJ&hl=en">Prof. Jiwen Lu</a>)</td>
                  </tr>
                  <tr>			
                  <td width= "400">Yiji Cheng (2022-)</td>
	          <td width= "400">Jinpeng Liu (2022-)</td>
                  </tr>
                  <tr>				  
		  <td width= "400">Aoyang Liu (2022-)</td>
		  <td width= "400">Sujia Wang (2022-)</td>
                  </tr>
                  <tr>				  
		  <td width= "400">Yunzhi Teng (2022-)</td>
		  <td width= "400">Wenjia Geng (2022-; with <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en">Prof. Jie Zhou</a>)</td>		   
                  </tr>
                </table> 
	       </li>		    
            </p>
          </td>
        </tr>
      </tbody></table>	      
	          
	      
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Area Chair:</b> FG 2023
              </li>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer:</b> CVPR, ICCV, ECCV, AAAI and so on
              </li>		    
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b>  TPAMI, TIP, TMM, TCSVT and so on
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
	      
	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
<a href="https://clustrmaps.com/site/1bevc" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=pSqGxdyauRrVjsHq2YSTApeSzMqOewuzLgOt0g6Ow6Y&cl=ffffff" /></a>	          
       
      </td>
    </tr>
  </table>
</body>

</html>
